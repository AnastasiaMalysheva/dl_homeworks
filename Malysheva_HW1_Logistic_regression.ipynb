{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XML2DataFrame:\n",
    "    def __init__(self, xml_path):\n",
    "        xml_data = open(xml_path, encoding='utf-8')\n",
    "        self.root = ET.XML(xml_data.read())[1]\n",
    "\n",
    "    def parse_root(self, root):\n",
    "        return [self.parse_element(child) for child in iter(root)]\n",
    "\n",
    "    def parse_element(self, element, parsed=None):\n",
    "        if parsed is None:\n",
    "            parsed = dict()\n",
    "        for key in element.keys():\n",
    "            parsed[key] = element.attrib.get(key)\n",
    "        if element.text:\n",
    "            parsed[element.attrib[\"name\"]] = None if element.text == \"NULL\" else element.text\n",
    "        for child in list(element):\n",
    "            self.parse_element(child, parsed)\n",
    "        return parsed\n",
    "\n",
    "    def process_data(self):\n",
    "        structure_data = self.parse_root(self.root)\n",
    "        return pd.DataFrame(structure_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = XML2DataFrame(\"data/tkk_train_2016.xml\").process_data().fillna(0)\n",
    "train_bank = XML2DataFrame(\"data/bank_train_2016.xml\").process_data().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = XML2DataFrame(\"data/tkk_test_etalon.xml\").process_data().fillna(0)\n",
    "test_bank = XML2DataFrame(\"data/banks_test_etalon.xml\").process_data().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test, form object vector and labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = train['text'].values\n",
    "test_text = test['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train[[\"beeline\", \"komstar\", \"mts\", \"rostelecom\", \"skylink\", \"tele2\"]].astype(int).sum(axis=1).values\n",
    "test_labels = test[[\"beeline\", \"komstar\", \"mts\", \"rostelecom\", \"skylink\", \"tele2\"]].astype(int).sum(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bank_text = train_bank['text'].values\n",
    "test_bank_text = test_bank['text'].values\n",
    "train_bank_labels = train_bank[['alfabank','bankmoskvy','gazprom','raiffeisen','rshb','sberbank','uralsib','vtb']].astype(int).sum(axis=1).values\n",
    "test_bank_labels = test_bank[['alfabank','bankmoskvy','gazprom','raiffeisen','rshb','sberbank','uralsib','vtb']].astype(int).sum(axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text - tokenize, delete stop-words, stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('russian')\n",
    "mystem = SnowballStemmer('russian')\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    tokens = [mystem.stem(token) for token in tokens if token not in stop]\n",
    "    \n",
    "    #text = \" \".join(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ready = [preprocess_text(text) for text in train_text]\n",
    "test_ready = [preprocess_text(text) for text in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bank_ready = [preprocess_text(text) for text in train_bank_text]\n",
    "test_bank_ready = [preprocess_text(text) for text in test_bank_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec and Tfidf models on both train and test texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-15 18:04:08,369 : INFO : collecting all words and their counts\n",
      "2018-10-15 18:04:08,372 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-15 18:04:08,420 : INFO : collected 14924 word types from a corpus of 91501 raw words and 8643 sentences\n",
      "2018-10-15 18:04:08,421 : INFO : Loading a fresh vocabulary\n",
      "2018-10-15 18:04:08,448 : INFO : effective_min_count=25 retains 520 unique words (3% of original 14924, drops 14404)\n",
      "2018-10-15 18:04:08,450 : INFO : effective_min_count=25 leaves 56280 word corpus (61% of original 91501, drops 35221)\n",
      "2018-10-15 18:04:08,460 : INFO : deleting the raw counts dictionary of 14924 items\n",
      "2018-10-15 18:04:08,462 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2018-10-15 18:04:08,464 : INFO : downsampling leaves estimated 36859 word corpus (65.5% of prior 56280)\n",
      "2018-10-15 18:04:08,469 : INFO : estimated required memory for 520 words and 100 dimensions: 676000 bytes\n",
      "2018-10-15 18:04:08,471 : INFO : resetting layer weights\n",
      "2018-10-15 18:04:08,490 : INFO : training model with 3 workers on 520 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-15 18:04:08,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:08,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:08,574 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:08,578 : INFO : EPOCH - 1 : training on 91501 raw words (36820 effective words) took 0.1s, 576537 effective words/s\n",
      "2018-10-15 18:04:08,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:08,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:08,658 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:08,661 : INFO : EPOCH - 2 : training on 91501 raw words (36919 effective words) took 0.1s, 556216 effective words/s\n",
      "2018-10-15 18:04:08,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:08,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:08,742 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:08,744 : INFO : EPOCH - 3 : training on 91501 raw words (36800 effective words) took 0.1s, 544930 effective words/s\n",
      "2018-10-15 18:04:08,826 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:08,829 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:08,835 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:08,836 : INFO : EPOCH - 4 : training on 91501 raw words (37017 effective words) took 0.1s, 488189 effective words/s\n",
      "2018-10-15 18:04:08,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:08,909 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:08,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:08,915 : INFO : EPOCH - 5 : training on 91501 raw words (36735 effective words) took 0.1s, 578012 effective words/s\n",
      "2018-10-15 18:04:08,918 : INFO : training on a 457505 raw words (184291 effective words) took 0.4s, 434488 effective words/s\n",
      "2018-10-15 18:04:08,920 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-10-15 18:04:08,922 : INFO : training model with 3 workers on 520 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-15 18:04:08,952 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:08,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:08,961 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:08,964 : INFO : EPOCH - 1 : training on 26039 raw words (10240 effective words) took 0.0s, 610003 effective words/s\n",
      "2018-10-15 18:04:08,968 : WARNING : EPOCH - 1 : supplied example count (2247) did not equal expected count (8643)\n",
      "2018-10-15 18:04:08,999 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:09,002 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:09,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:09,007 : INFO : EPOCH - 2 : training on 26039 raw words (10196 effective words) took 0.0s, 432917 effective words/s\n",
      "2018-10-15 18:04:09,009 : WARNING : EPOCH - 2 : supplied example count (2247) did not equal expected count (8643)\n",
      "2018-10-15 18:04:09,040 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:09,047 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:09,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:09,053 : INFO : EPOCH - 3 : training on 26039 raw words (10153 effective words) took 0.0s, 306531 effective words/s\n",
      "2018-10-15 18:04:09,058 : WARNING : EPOCH - 3 : supplied example count (2247) did not equal expected count (8643)\n",
      "2018-10-15 18:04:09,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:09,092 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:09,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:09,097 : INFO : EPOCH - 4 : training on 26039 raw words (10256 effective words) took 0.0s, 447357 effective words/s\n",
      "2018-10-15 18:04:09,099 : WARNING : EPOCH - 4 : supplied example count (2247) did not equal expected count (8643)\n",
      "2018-10-15 18:04:09,125 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:09,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:09,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:09,139 : INFO : EPOCH - 5 : training on 26039 raw words (10227 effective words) took 0.0s, 345590 effective words/s\n",
      "2018-10-15 18:04:09,143 : WARNING : EPOCH - 5 : supplied example count (2247) did not equal expected count (8643)\n",
      "2018-10-15 18:04:09,146 : INFO : training on a 130195 raw words (51072 effective words) took 0.2s, 230768 effective words/s\n",
      "2018-10-15 18:04:09,148 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(51072, 130195)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(text_ready, min_count=25, size=100)\n",
    "model.train(test_ready, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-15 18:04:09,657 : INFO : collecting all words and their counts\n",
      "2018-10-15 18:04:09,660 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-15 18:04:09,710 : INFO : collected 15035 word types from a corpus of 96267 raw words and 9392 sentences\n",
      "2018-10-15 18:04:09,712 : INFO : Loading a fresh vocabulary\n",
      "2018-10-15 18:04:09,733 : INFO : effective_min_count=25 retains 371 unique words (2% of original 15035, drops 14664)\n",
      "2018-10-15 18:04:09,737 : INFO : effective_min_count=25 leaves 65977 word corpus (68% of original 96267, drops 30290)\n",
      "2018-10-15 18:04:09,744 : INFO : deleting the raw counts dictionary of 15035 items\n",
      "2018-10-15 18:04:09,748 : INFO : sample=0.001 downsamples 42 most-common words\n",
      "2018-10-15 18:04:09,751 : INFO : downsampling leaves estimated 29880 word corpus (45.3% of prior 65977)\n",
      "2018-10-15 18:04:09,757 : INFO : estimated required memory for 371 words and 100 dimensions: 482300 bytes\n",
      "2018-10-15 18:04:09,759 : INFO : resetting layer weights\n",
      "2018-10-15 18:04:09,777 : INFO : training model with 3 workers on 371 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-15 18:04:09,853 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:09,855 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:09,859 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:09,862 : INFO : EPOCH - 1 : training on 96267 raw words (29780 effective words) took 0.1s, 438350 effective words/s\n",
      "2018-10-15 18:04:09,935 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:09,940 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:09,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:09,945 : INFO : EPOCH - 2 : training on 96267 raw words (29936 effective words) took 0.1s, 444115 effective words/s\n",
      "2018-10-15 18:04:10,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:10,011 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:10,014 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:10,017 : INFO : EPOCH - 3 : training on 96267 raw words (29859 effective words) took 0.1s, 524453 effective words/s\n",
      "2018-10-15 18:04:10,088 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:10,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:10,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:10,097 : INFO : EPOCH - 4 : training on 96267 raw words (29693 effective words) took 0.1s, 450798 effective words/s\n",
      "2018-10-15 18:04:10,166 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:10,170 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:10,173 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:10,176 : INFO : EPOCH - 5 : training on 96267 raw words (29979 effective words) took 0.1s, 454644 effective words/s\n",
      "2018-10-15 18:04:10,180 : INFO : training on a 481335 raw words (149247 effective words) took 0.4s, 372992 effective words/s\n",
      "2018-10-15 18:04:10,183 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-10-15 18:04:10,186 : INFO : training model with 3 workers on 371 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-10-15 18:04:10,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:10,229 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:10,233 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:10,236 : INFO : EPOCH - 1 : training on 38749 raw words (12243 effective words) took 0.0s, 376588 effective words/s\n",
      "2018-10-15 18:04:10,239 : WARNING : EPOCH - 1 : supplied example count (3313) did not equal expected count (8643)\n",
      "2018-10-15 18:04:10,277 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:10,282 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:10,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:10,287 : INFO : EPOCH - 2 : training on 38749 raw words (12066 effective words) took 0.0s, 352838 effective words/s\n",
      "2018-10-15 18:04:10,293 : WARNING : EPOCH - 2 : supplied example count (3313) did not equal expected count (8643)\n",
      "2018-10-15 18:04:10,335 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:10,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:10,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:10,349 : INFO : EPOCH - 3 : training on 38749 raw words (12170 effective words) took 0.0s, 384743 effective words/s\n",
      "2018-10-15 18:04:10,351 : WARNING : EPOCH - 3 : supplied example count (3313) did not equal expected count (8643)\n",
      "2018-10-15 18:04:10,381 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:10,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:10,388 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:10,390 : INFO : EPOCH - 4 : training on 38749 raw words (12190 effective words) took 0.0s, 506418 effective words/s\n",
      "2018-10-15 18:04:10,393 : WARNING : EPOCH - 4 : supplied example count (3313) did not equal expected count (8643)\n",
      "2018-10-15 18:04:10,427 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-15 18:04:10,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-15 18:04:10,440 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-15 18:04:10,443 : INFO : EPOCH - 5 : training on 38749 raw words (12159 effective words) took 0.0s, 369676 effective words/s\n",
      "2018-10-15 18:04:10,446 : WARNING : EPOCH - 5 : supplied example count (3313) did not equal expected count (8643)\n",
      "2018-10-15 18:04:10,449 : INFO : training on a 193745 raw words (60828 effective words) took 0.3s, 232252 effective words/s\n",
      "2018-10-15 18:04:10,452 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60828, 193745)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bank = gensim.models.Word2Vec(train_bank_ready, min_count=25, size=100)\n",
    "model_bank.train(test_bank_ready, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "vect.fit([' '.join(text) for text in train_ready])\n",
    "vect.fit([' '.join(text) for text in test_ready])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_bank = TfidfVectorizer()\n",
    "vect_bank.fit([' '.join(text) for text in train_bank_ready])\n",
    "vect_bank.fit([' '.join(text) for text in test_bank_ready])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train logistic regression on tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = vect.transform([' '.join(text) for text in train_ready])\n",
    "X_test = vect.transform([' '.join(text) for text in test_ready])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Accuracy on tfidf for tkk: ', 0.6386292834890965)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train, train_labels)\n",
    "pred_labels = lr.predict(X_test)\n",
    "'Accuracy on tfidf for tkk: ', accuracy_score(test_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_bank_train = vect_bank.transform([' '.join(text) for text in train_bank_ready])\n",
    "X_bank_test = vect_bank.transform([' '.join(text) for text in test_bank_ready])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Accuracy on tfidf for bank: ', 0.7398128584364624)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bank = LogisticRegression().fit(X_bank_train, train_bank_labels)\n",
    "pred_bank_labels = lr_bank.predict(X_bank_test)\n",
    "'Accuracy on tfidf for bank: ', accuracy_score(test_bank_labels, pred_bank_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature matrix. Each text is a sum of word vectors weighted by idf coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_features_of_text(text, feature_names, model):\n",
    "    result_vect = np.zeros(model.layer1_size)\n",
    "    for word in text:\n",
    "        try:\n",
    "            result_vect += model.wv[word]*vect.idf_[feature_names.index(word)]\n",
    "        except:\n",
    "            pass\n",
    "    return result_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `layer1_size` (Attribute will be removed in 4.0.0, use self.trainables.layer1_size instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "X_train = np.array([get_features_of_text(text, feature_names, model) for text in train_ready])\n",
    "X_text = np.array([get_features_of_text(text, feature_names, model) for text in test_ready])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `layer1_size` (Attribute will be removed in 4.0.0, use self.trainables.layer1_size instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect_bank.get_feature_names()\n",
    "\n",
    "X_bank_train = np.array([get_features_of_text(text, feature_names, model_bank) for text in train_bank_ready])\n",
    "X_bank_text = np.array([get_features_of_text(text, feature_names, model_bank) for text in test_bank_ready])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit logistic regression, estimate quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Accuracy on word2vec-idf for tkk: ', 0.5763239875389408)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, train_labels)\n",
    "y_pred = lr.predict(X_text)\n",
    "'Accuracy on word2vec-idf for tkk: ', accuracy_score(y_pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Accuracy on word2vec-idf for bank: ', 0.6939329912466042)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bank = LogisticRegression()\n",
    "lr_bank.fit(X_bank_train, train_bank_labels)\n",
    "y_bank_pred = lr_bank.predict(X_bank_text)\n",
    "'Accuracy on word2vec-idf for bank: ', accuracy_score(y_bank_pred, test_bank_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
